{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation_eng_hindi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TphUnHWf_52Y"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk.translate.bleu_score as bleu\n",
        "import random\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "X4Opm0xU_7OX",
        "outputId": "b3fc38db-3d43-4a50-df3c-114d96db30f6"
      },
      "source": [
        "eng_hin=pd.read_csv('eng_hin.csv')\n",
        "eng_hin.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ted</td>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ted</td>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ted</td>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      source  ...                                     hindi_sentence\n",
              "0        ted  ...  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...\n",
              "1        ted  ...  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...\n",
              "2  indic2012  ...   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
              "3        ted  ...     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
              "4  indic2012  ...        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yQUqA6g_8lh",
        "outputId": "dc97d2ec-907a-4533-d03c-907af3fc3d92"
      },
      "source": [
        "eng_hin.dropna(inplace=True)\n",
        "eng_hin=eng_hin[:50000]\n",
        "eng_hin.drop(['source'],axis=1,inplace=True)\n",
        "eng_hin.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlhv9yF1AIBf"
      },
      "source": [
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeQ2I1iDAJ5o"
      },
      "source": [
        "def preprocess(text):\n",
        "    '''Function to preprocess English sentence'''\n",
        "    text = text.lower() # lower casing\n",
        "    text = re.sub(\"'\", '', text) # remove the quotation marks if any\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    text = text.translate(remove_digits) # remove the digits\n",
        "    text = text.strip()\n",
        "    text = re.sub(\" +\", \" \", text) # remove extra spaces\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQcdfXMDALjr"
      },
      "source": [
        "def preprocess_hin(text):\n",
        "    '''Function to preprocess Marathi sentence'''\n",
        "    text = re.sub(\"'\", '', text) # remove the quotation marks if any\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    text = re.sub(\"[२३०८१५७९४६]\", \"\", text) # remove the digits\n",
        "    text = text.strip()\n",
        "    text = re.sub(\" +\", \" \", text) # remove extra spaces\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uYYoa6mfAc0Q",
        "outputId": "649dcd7e-0629-4d68-d28f-812b630e2efa"
      },
      "source": [
        "eng_hin['english_sentence'] = eng_hin['english_sentence'].apply(preprocess)\n",
        "eng_hin['hindi_sentence'] = eng_hin['hindi_sentence'].apply(preprocess_hin)\n",
        "\n",
        "eng_hin.rename(columns={\"english_sentence\": \"english\", \"hindi_sentence\": \"hindi\"},inplace=True)\n",
        "\n",
        "eng_hin.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;start&gt; politicians do not have permission to ...</td>\n",
              "      <td>&lt;start&gt; राजनीतिज्ञों के पास जो कार्य करना चाहि...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;start&gt; id like to tell you about one such chi...</td>\n",
              "      <td>&lt;start&gt; मई आपको ऐसे ही एक बच्चे के बारे में बत...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;start&gt; this percentage is even greater than t...</td>\n",
              "      <td>&lt;start&gt; यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;start&gt; what we really mean is that theyre bad...</td>\n",
              "      <td>&lt;start&gt; हम ये नहीं कहना चाहते कि वो ध्यान नहीं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;start&gt; the ending portion of these vedas is c...</td>\n",
              "      <td>&lt;start&gt; इन्हीं वेदों का अंतिम भाग उपनिषद कहलात...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             english                                              hindi\n",
              "0  <start> politicians do not have permission to ...  <start> राजनीतिज्ञों के पास जो कार्य करना चाहि...\n",
              "1  <start> id like to tell you about one such chi...  <start> मई आपको ऐसे ही एक बच्चे के बारे में बत...\n",
              "2  <start> this percentage is even greater than t...  <start> यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...\n",
              "3  <start> what we really mean is that theyre bad...  <start> हम ये नहीं कहना चाहते कि वो ध्यान नहीं...\n",
              "4  <start> the ending portion of these vedas is c...  <start> इन्हीं वेदों का अंतिम भाग उपनिषद कहलात..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zNS9JknEWfa"
      },
      "source": [
        "def tokenize(lang):\n",
        "\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post',maxlen=20,dtype='int32')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQkH882Fs9x"
      },
      "source": [
        "def load_dataset():\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(eng_hin['english'].values)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(eng_hin['hindi'].values)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Wg24AOFO9s"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUvO_uSiGCLC"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G9J9aanGEnY",
        "outputId": "a827033a-e78c-43fa-80df-9801a7c7aa54"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000 40000 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9oyMQjLGZ4x"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "vocab_inp_size =len(inp_lang.word_index.keys())\n",
        "vocab_tar_size =len(targ_lang.word_index.keys())\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKD-3jidQtND"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_inp_size+1, 300))\n",
        "for word, i in inp_lang.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_-Bj6ENHE4I"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer_encoder\",trainable=False)\n",
        "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afDryQ94HdKt"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "                # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oGe0qnwHyA5"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "encoder = Encoder(vocab_inp_size+1, 300, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size+1, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV6fzW6vQzXd"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p3etBjuQ2M6"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf-qboRWQ4nC"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    encoder.get_layer('embedding_layer_encoder').set_weights([embedding_matrix])\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNTA90Y5UfeG",
        "outputId": "bd6fe673-1f6b-4d0a-9993-9bc72bf858e0"
      },
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.3141\n",
            "Epoch 1 Batch 100 Loss 5.1003\n",
            "Epoch 1 Batch 200 Loss 4.6859\n",
            "Epoch 1 Batch 300 Loss 4.5698\n",
            "Epoch 1 Batch 400 Loss 4.3429\n",
            "Epoch 1 Batch 500 Loss 4.8458\n",
            "Epoch 1 Batch 600 Loss 4.0618\n",
            "Epoch 1 Loss 4.6826\n",
            "Time taken for 1 epoch 135.76 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.3634\n",
            "Epoch 2 Batch 100 Loss 4.3127\n",
            "Epoch 2 Batch 200 Loss 3.5064\n",
            "Epoch 2 Batch 300 Loss 4.2723\n",
            "Epoch 2 Batch 400 Loss 3.9200\n",
            "Epoch 2 Batch 500 Loss 3.5746\n",
            "Epoch 2 Batch 600 Loss 3.5531\n",
            "Epoch 2 Loss 3.9863\n",
            "Time taken for 1 epoch 119.40 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.5205\n",
            "Epoch 3 Batch 100 Loss 3.8863\n",
            "Epoch 3 Batch 200 Loss 3.3188\n",
            "Epoch 3 Batch 300 Loss 3.5382\n",
            "Epoch 3 Batch 400 Loss 3.2281\n",
            "Epoch 3 Batch 500 Loss 3.2979\n",
            "Epoch 3 Batch 600 Loss 3.1598\n",
            "Epoch 3 Loss 3.4068\n",
            "Time taken for 1 epoch 114.08 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.8931\n",
            "Epoch 4 Batch 100 Loss 2.8633\n",
            "Epoch 4 Batch 200 Loss 2.7618\n",
            "Epoch 4 Batch 300 Loss 2.9703\n",
            "Epoch 4 Batch 400 Loss 2.9389\n",
            "Epoch 4 Batch 500 Loss 3.0428\n",
            "Epoch 4 Batch 600 Loss 2.6770\n",
            "Epoch 4 Loss 2.8328\n",
            "Time taken for 1 epoch 118.85 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.5234\n",
            "Epoch 5 Batch 100 Loss 2.4262\n",
            "Epoch 5 Batch 200 Loss 2.2987\n",
            "Epoch 5 Batch 300 Loss 2.1696\n",
            "Epoch 5 Batch 400 Loss 2.1769\n",
            "Epoch 5 Batch 500 Loss 1.9109\n",
            "Epoch 5 Batch 600 Loss 2.2626\n",
            "Epoch 5 Loss 2.2640\n",
            "Time taken for 1 epoch 115.11 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.6855\n",
            "Epoch 6 Batch 100 Loss 1.8560\n",
            "Epoch 6 Batch 200 Loss 1.7349\n",
            "Epoch 6 Batch 300 Loss 2.0522\n",
            "Epoch 6 Batch 400 Loss 1.9474\n",
            "Epoch 6 Batch 500 Loss 1.8774\n",
            "Epoch 6 Batch 600 Loss 1.8841\n",
            "Epoch 6 Loss 1.7824\n",
            "Time taken for 1 epoch 118.92 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2763\n",
            "Epoch 7 Batch 100 Loss 1.2425\n",
            "Epoch 7 Batch 200 Loss 1.4230\n",
            "Epoch 7 Batch 300 Loss 1.3129\n",
            "Epoch 7 Batch 400 Loss 1.4031\n",
            "Epoch 7 Batch 500 Loss 1.3770\n",
            "Epoch 7 Batch 600 Loss 1.4774\n",
            "Epoch 7 Loss 1.4442\n",
            "Time taken for 1 epoch 114.87 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0729\n",
            "Epoch 8 Batch 100 Loss 1.0095\n",
            "Epoch 8 Batch 200 Loss 1.1226\n",
            "Epoch 8 Batch 300 Loss 1.1227\n",
            "Epoch 8 Batch 400 Loss 1.2757\n",
            "Epoch 8 Batch 500 Loss 1.1420\n",
            "Epoch 8 Batch 600 Loss 1.4210\n",
            "Epoch 8 Loss 1.1598\n",
            "Time taken for 1 epoch 118.15 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9015\n",
            "Epoch 9 Batch 100 Loss 0.8456\n",
            "Epoch 9 Batch 200 Loss 0.8167\n",
            "Epoch 9 Batch 300 Loss 0.8959\n",
            "Epoch 9 Batch 400 Loss 0.8719\n",
            "Epoch 9 Batch 500 Loss 0.9965\n",
            "Epoch 9 Batch 600 Loss 0.9562\n",
            "Epoch 9 Loss 0.9190\n",
            "Time taken for 1 epoch 114.63 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5899\n",
            "Epoch 10 Batch 100 Loss 0.6748\n",
            "Epoch 10 Batch 200 Loss 0.7770\n",
            "Epoch 10 Batch 300 Loss 0.8876\n",
            "Epoch 10 Batch 400 Loss 0.7147\n",
            "Epoch 10 Batch 500 Loss 0.7851\n",
            "Epoch 10 Batch 600 Loss 0.7364\n",
            "Epoch 10 Loss 0.7197\n",
            "Time taken for 1 epoch 118.29 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3920\n",
            "Epoch 11 Batch 100 Loss 0.5170\n",
            "Epoch 11 Batch 200 Loss 0.6663\n",
            "Epoch 11 Batch 300 Loss 0.5698\n",
            "Epoch 11 Batch 400 Loss 0.4951\n",
            "Epoch 11 Batch 500 Loss 0.5837\n",
            "Epoch 11 Batch 600 Loss 0.6113\n",
            "Epoch 11 Loss 0.5557\n",
            "Time taken for 1 epoch 114.84 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.3407\n",
            "Epoch 12 Batch 100 Loss 0.4555\n",
            "Epoch 12 Batch 200 Loss 0.4133\n",
            "Epoch 12 Batch 300 Loss 0.3918\n",
            "Epoch 12 Batch 400 Loss 0.4309\n",
            "Epoch 12 Batch 500 Loss 0.4328\n",
            "Epoch 12 Batch 600 Loss 0.4697\n",
            "Epoch 12 Loss 0.4264\n",
            "Time taken for 1 epoch 118.48 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2731\n",
            "Epoch 13 Batch 100 Loss 0.2850\n",
            "Epoch 13 Batch 200 Loss 0.3745\n",
            "Epoch 13 Batch 300 Loss 0.3384\n",
            "Epoch 13 Batch 400 Loss 0.3718\n",
            "Epoch 13 Batch 500 Loss 0.3737\n",
            "Epoch 13 Batch 600 Loss 0.3630\n",
            "Epoch 13 Loss 0.3288\n",
            "Time taken for 1 epoch 115.36 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1904\n",
            "Epoch 14 Batch 100 Loss 0.2343\n",
            "Epoch 14 Batch 200 Loss 0.2468\n",
            "Epoch 14 Batch 300 Loss 0.2743\n",
            "Epoch 14 Batch 400 Loss 0.2214\n",
            "Epoch 14 Batch 500 Loss 0.2799\n",
            "Epoch 14 Batch 600 Loss 0.3353\n",
            "Epoch 14 Loss 0.2571\n",
            "Time taken for 1 epoch 118.25 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1821\n",
            "Epoch 15 Batch 100 Loss 0.1998\n",
            "Epoch 15 Batch 200 Loss 0.2141\n",
            "Epoch 15 Batch 300 Loss 0.2064\n",
            "Epoch 15 Batch 400 Loss 0.1849\n",
            "Epoch 15 Batch 500 Loss 0.2322\n",
            "Epoch 15 Batch 600 Loss 0.2316\n",
            "Epoch 15 Loss 0.2072\n",
            "Time taken for 1 epoch 117.31 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQLadYyExzp3",
        "outputId": "a24ea808-3bca-4aad-fdc3-27ebc02e3a70"
      },
      "source": [
        "for epoch in range(EPOCHS,20):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 16 Batch 0 Loss 0.1300\n",
            "Epoch 16 Batch 100 Loss 0.1290\n",
            "Epoch 16 Batch 200 Loss 0.1455\n",
            "Epoch 16 Batch 300 Loss 0.1636\n",
            "Epoch 16 Batch 400 Loss 0.2018\n",
            "Epoch 16 Batch 500 Loss 0.1756\n",
            "Epoch 16 Batch 600 Loss 0.2210\n",
            "Epoch 16 Loss 0.1711\n",
            "Time taken for 1 epoch 121.54 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1267\n",
            "Epoch 17 Batch 100 Loss 0.0918\n",
            "Epoch 17 Batch 200 Loss 0.1483\n",
            "Epoch 17 Batch 300 Loss 0.1260\n",
            "Epoch 17 Batch 400 Loss 0.1401\n",
            "Epoch 17 Batch 500 Loss 0.1921\n",
            "Epoch 17 Batch 600 Loss 0.1564\n",
            "Epoch 17 Loss 0.1470\n",
            "Time taken for 1 epoch 115.51 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0954\n",
            "Epoch 18 Batch 100 Loss 0.1427\n",
            "Epoch 18 Batch 200 Loss 0.1297\n",
            "Epoch 18 Batch 300 Loss 0.1105\n",
            "Epoch 18 Batch 400 Loss 0.1601\n",
            "Epoch 18 Batch 500 Loss 0.1347\n",
            "Epoch 18 Batch 600 Loss 0.1469\n",
            "Epoch 18 Loss 0.1283\n",
            "Time taken for 1 epoch 118.97 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1313\n",
            "Epoch 19 Batch 100 Loss 0.1307\n",
            "Epoch 19 Batch 200 Loss 0.1305\n",
            "Epoch 19 Batch 300 Loss 0.1055\n",
            "Epoch 19 Batch 400 Loss 0.1299\n",
            "Epoch 19 Batch 500 Loss 0.1442\n",
            "Epoch 19 Batch 600 Loss 0.1235\n",
            "Epoch 19 Loss 0.1190\n",
            "Time taken for 1 epoch 115.15 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1088\n",
            "Epoch 20 Batch 100 Loss 0.1030\n",
            "Epoch 20 Batch 200 Loss 0.0908\n",
            "Epoch 20 Batch 300 Loss 0.1122\n",
            "Epoch 20 Batch 400 Loss 0.0974\n",
            "Epoch 20 Batch 500 Loss 0.1559\n",
            "Epoch 20 Batch 600 Loss 0.1532\n",
            "Epoch 20 Loss 0.1150\n",
            "Time taken for 1 epoch 118.74 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0sehhJFjZhE"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=20, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result,attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result,attention_plot"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9pfZPH2Rl2o",
        "outputId": "160aa52e-43ce-4c25-bbb3-a6e1b03c899c"
      },
      "source": [
        "input_sentence= 'please ensure that you use the appropriate form '\n",
        "print('Input sentence in english : ',input_sentence)\n",
        "predicted_output,attention_plot=evaluate(input_sentence)\n",
        "print('Predicted sentence in hindi : ',predicted_output)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sentence in english :  please ensure that you use the appropriate form \n",
            "Predicted sentence in hindi :  कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प्रयोग कर रहें हैं <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z2NmL-vTCDr",
        "outputId": "b329e2f8-8285-45d4-8311-88e8f5732b1b"
      },
      "source": [
        "input_sentence='and do something with it to change the world '\n",
        "print('Input sentence in english : ',input_sentence)\n",
        "predicted_output,attention_plot=evaluate(input_sentence)\n",
        "print('Predicted sentence in hindi : ',predicted_output)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sentence in english :  and do something with it to change the world \n",
            "Predicted sentence in hindi :  और इस दुनिया को बेहतर बनाने के लिये कुछ करेंगे । <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}